{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"style/style.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" href=\"style/style.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lake model continued\n",
    "\n",
    "In the previous week you used the lake problem as a means of getting aquinted with the workbench. In this assignment we will continue with the lake problem, focussing explicitly on using it for open exploration. You can use the second part of [this tutorial](https://emaworkbench.readthedocs.io/en/latest/indepth_tutorial/open-exploration.html) for help.\n",
    "\n",
    "**It is paramount that you are using the lake problem with 100 decision variables, rather than the one found on the website with the seperate anthropogenic release decision**\n",
    "\n",
    "## Apply scenario discovery\n",
    "\n",
    "1. Generate 10 policies and 1000 scenarios and evaluate them.\n",
    "2. The experiments array contains the values for each of the 100 decision levers. This might easily mess up the analysis. Remove these columns from the experiment array. *hint: use `experiments.drop`*\n",
    "3. Apply scenario discovery, focussing on the 10 percent of worst outcomes for reliability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakemodel_function import lake_problem\n",
    "\n",
    "from ema_workbench import Model, RealParameter, ScalarOutcome, SequentialEvaluator, IpyparallelEvaluator, ema_logging, perform_experiments, MultiprocessingEvaluator\n",
    "\n",
    "#instantiate the model\n",
    "lake_model = Model('lakeproblem', function=lake_problem)\n",
    "lake_model.time_horizon = 100 # used to specify the number of timesteps\n",
    "\n",
    "#specify uncertainties\n",
    "lake_model.uncertainties = [RealParameter('mean', 0.01, 0.05),\n",
    "                            RealParameter('stdev', 0.001, 0.005),\n",
    "                            RealParameter('b', 0.1, 0.45),\n",
    "                            RealParameter('q', 2.0, 4.5),\n",
    "                            RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "# set levers, one for each time step\n",
    "lake_model.levers = [RealParameter(f\"l{i}\", 0, 0.1) for i in \n",
    "                     range(lake_model.time_horizon)] # we use time_horizon here\n",
    "\n",
    "#specify outcomes \n",
    "lake_model.outcomes = [ScalarOutcome('max_P'),\n",
    "                       ScalarOutcome('utility'),\n",
    "                       ScalarOutcome('inertia'),\n",
    "                       ScalarOutcome('reliability')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 1000 scenarios * 10 policies * 1 model(s) = 10000 experiments\n"
     ]
    }
   ],
   "source": [
    "from ema_workbench import Policy, perform_experiments\n",
    "from ema_workbench import ema_logging\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "n_scenarios = 1000\n",
    "n_policies = 10\n",
    "from ema_workbench import MultiprocessingEvaluator\n",
    "\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios, n_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "experiments, outcomes = results\n",
    "policies = experiments['policy']\n",
    "\n",
    "data = pd.DataFrame.from_dict(outcomes)\n",
    "data['policy'] = policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cleaned = experiments.drop(experiments.iloc[:, 5:-3], axis=1)\n",
    "exp_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario discovery\n",
    "from ema_workbench.analysis import prim\n",
    "import numpy as np\n",
    "\n",
    "x = exp_cleaned\n",
    "y = outcomes['reliability'] < np.percentile(outcomes['reliability'], 10)\n",
    "\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8)\n",
    "box1 = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "box1.show_tradeoff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows tradeoff between coverage of the box (how many points of interest are within the PRIM box) and density (ratio between points of interest and points not of interest). \n",
    "When 0/1 dimension is restricted (blue) there is high coverage, but very low density (max 0.2).\n",
    "When 2 dimensions are restricted (green) density increase with slow relative decrease in coverage.\n",
    "When 3 dimensions are restricted (yellow) there is greater relative improvement in density for smaller losses in coverages. \n",
    "\n",
    "_We are unsure how to relate this back to the lakemodel iteself, what counts as a point of interest?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect individual point\n",
    "box1.inspect(20)\n",
    "box1.inspect(20, style='graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of interest point 20 was chosen due to its high coverage and fewer restricted parameters. Quasi p-values indicate that variation in output is statistically significant by modifying the variable in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.show_pairs_scatter(12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True indicates points at which reliability exceedes the threshold and leads to irrevesible eutrophication. We observe that these generally occur when both b and q are low. These refer to the lake's natural removal and recycling rate respectively, which if both low would quickly lead to accumulation of pollutants and lowering reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results using Dimensional Stacking\n",
    "Take the classification of outcomes as used in step 3 of scenario discovery, and instead visualize the results using dimensional stacking. How do these results compare to the insights from scenario discovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_array = pd.DataFrame(outcomes)\n",
    "\n",
    "outcomes_cleaned = outcomes_array[outcomes_array.reliability < np.percentile(outcomes['reliability'], 10)]\n",
    "outcomes_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_test = exp_cleaned[exp_cleaned.index.isin(outcomes_cleaned.index)]\n",
    "exp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_new = outcomes_cleaned.to_dict(orient = 'list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visually identify the uncertainties that drive system behaviour\n",
    "import seaborn as sns\n",
    "\n",
    "from ema_workbench.analysis import feature_scoring\n",
    " \n",
    "x = exp_test\n",
    "y = outcomes_new #['reliability'] #< np.percentile(outcomes['reliability'], 10)\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "fs = feature_scoring.get_feature_scores_all(x, y)\n",
    "sns.heatmap(fs, cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dimensional stacking assessment reveals that the uncertainty $b$ (the lake's natural removal rate) has the largest impact on the outcome for maximum phosphorous concentration. This is followed closely by the mean (mean of natural inflows into the lake) which influences the reliability of lake water quality. The other three uncertainties have less significant impact on the outcomes of interest. This suggests that these two features (which effectively represent natural inflows and outflows from the lake) are more likely to drive system behaviour than other quantified uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.7.9"
=======
   "version": "3.7.8"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
