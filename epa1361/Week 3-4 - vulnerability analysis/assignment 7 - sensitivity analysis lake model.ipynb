{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lake model continued\n",
    "\n",
    "In the previous week you used the lake problem as a means of getting aquinted with the workbench. In this assignment we will continue with the lake problem, focussing explicitly on using it for open exploration. You can use the second part of [this tutorial](https://emaworkbench.readthedocs.io/en/latest/indepth_tutorial/open-exploration.html) for help.\n",
    "\n",
    "**It is paramount that you are using the lake problem with 100 decision variables, rather than the one found on the website with the seperate anthropogenic release decision**\n",
    "\n",
    "## Apply sensitivity analysis\n",
    "There is substantial support in the ema_workbench for global sensitivity. For this, the workbench relies on [SALib](https://salib.readthedocs.io/en/latest/) and feature scoring which is a machine learning alternative for global sensitivity analysis. \n",
    "\n",
    "\n",
    "1. Apply Sobol with 3 seperate release policies (0, 0.05, and 0.1) and analyse the results for each release policy seperately focusing on the reliability objective. Do the sensitivities change depending on the release policy? Can you explain why or why not?\n",
    "\n",
    "*hint: you can use sobol sampling for the uncertainties, and set policies to a list with the 3 different release policies. Next, for the analysis using logical indexing on the experiment.policy column you can select the results for each seperate release policy and apply sobol to each of the three seperate release policies. If this sounds too complicated, just do it on each release policy seperately.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Sobol sampling functionality included in the Workbench to perform experiments with a sample size of N=50, then analyze the results with SALib for the same three indicators. This requires specifying the keyword argument 'uncertainty_sampling' of perform_experiments. Note that when using Sobol sampling, the meaning of the keyword argument scenarios changes a bit. In order to properly estimate Sobol scores as well as interaction effects, you require N * (2D+2) scenarios, where D is the number of uncertain parameters, and N is the value for scenarios passed to perform_experiments. Repeat the analysis for larger sample sizes, with N=250 and N=1000. How can we interpret the first-order and total indices? Are these sample sizes sufficient for a stable estimation of the indices? You'll need to use the get_SALib_problem function to convert your Workbench experiments to a problem definition that you can pass to the SALib analysis function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from ema_workbench import (Model, RealParameter, Constant, ScalarOutcome, TimeSeriesOutcome, perform_experiments, ema_logging, SequentialEvaluator, MultiprocessingEvaluator)\n",
    "from ema_workbench.em_framework.evaluators import LHS, SOBOL, MORRIS\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from ema_workbench.analysis.scenario_discovery_util import RuleInductionType\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from SALib.analyze import sobol\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "\n",
    "import lakemodel_function as lake\n",
    "\n",
    "model = Model('Lakemodel', function=lake.lake_problem)\n",
    "\n",
    "uncertainties = [RealParameter('mean', 0.01, 0.5),\n",
    "                 RealParameter('stdev', 0.001, 0.005),\n",
    "                 RealParameter('b', 0.1, 0.45),\n",
    "                 RealParameter('q', 2, 4.5),\n",
    "                 RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "outcomes = [ScalarOutcome('max_P', ScalarOutcome.MINIMIZE),\n",
    "            ScalarOutcome('utility', ScalarOutcome.MAXIMIZE),\n",
    "            ScalarOutcome('inertia', ScalarOutcome.INFO),\n",
    "            ScalarOutcome('reliability', ScalarOutcome.MAXIMIZE)]\n",
    "# assumption that inertia does not matter and is only there to provide info.\n",
    "\n",
    "model.uncertainties = uncertainties\n",
    "\n",
    "# model.levers = [RealParameter(\"l\" + str(number),0,0.1) for number in range(0,100)]\n",
    "\n",
    "model.outcomes = outcomes\n",
    "\n",
    "# 3 seperate release policies (0, 0.05, and 0.1)\n",
    "# TODO: pass as a list for ease after initial testing succeeds\n",
    "model.levers = [Constant('l' + str(i), 0) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 1000 scenarios * 1 policies * 1 model(s) = 1000 experiments\n"
     ]
    }
   ],
   "source": [
    "# Model run\n",
    "processes = 8\n",
    "\n",
    "#ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(model, processes) as evaluator:\n",
    "    # I think no policies because levers is a constant?\n",
    "    experiments, outcomes = evaluator.perform_experiments(scenarios=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repeat the above analysis for the 3 release policies but now with extra trees feature scoring and for all outcomes of interest. As a bonus, use the sobol experiment results as input for extra trees, and compare the results with those resulting from latin hypercube sampling.\n",
    "\n",
    "*hint: you can use [seaborn heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for a nice figure of the results*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
