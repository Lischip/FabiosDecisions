{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import Scenario, Policy, MultiprocessingEvaluator, ema_logging, load_results,  save_results\n",
    "from ema_workbench.analysis import prim\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume,\n",
    "                                                     EpsilonProgress)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ema_workbench.analysis import parcoords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deventer Expected Annual Damage',\n",
       " 'Deventer Expected Number of Deaths',\n",
       " 'Deventer Budget Overrun',\n",
       " 'Deventer Total Costs']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTORNAME = \"Deventer\"\n",
    "ACTOR = \"genscen_Deventer_1000_11-06-2021-13-45-33.tar.gz\"\n",
    "\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(ACTORNAME)\n",
    "outcomekeys = [outcome.name for outcome in dike_model.outcomes]\n",
    "outcomekeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = load_results(\"data/generated/\" + ACTOR)\n",
    "outcomes_df = pd.DataFrame(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deventer Expected Annual Damage',\n",
       " 'Deventer Expected Number of Deaths',\n",
       " 'Deventer Budget Overrun',\n",
       " 'Deventer Total Costs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAMAGE = outcomes_df.columns[0]\n",
    "DEATHS = outcomes_df.columns[1]\n",
    "COSTS = outcomes_df.columns[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([experiments, outcomes_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_damage_df = results.loc[results[DAMAGE] > np.percentile(a=outcomes[DAMAGE], q=90)]\n",
    "worst_deaths_df = results.loc[results[DEATHS] > np.percentile(a=outcomes[DEATHS], q=90)]\n",
    "worst_cost_df = results.loc[results[COSTS] > np.percentile(a=outcomes[COSTS], q=90)]\n",
    "worst_ix =set(worst_damage_df[\"scenario\"].tolist()) & set(worst_deaths_df[\"scenario\"].tolist()) # & set(worst_cost_df[\"scenario\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_case = results.iloc[list(worst_ix)].sample(n=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_damage_df = results.loc[results[DAMAGE] <= np.percentile(a=outcomes[DAMAGE], q=10)]\n",
    "best_deaths_df = results.loc[results[DEATHS] <= np.percentile(a=outcomes[DEATHS], q=10)]\n",
    "best_cost_df = results.loc[results[COSTS] <= np.percentile(a=outcomes[COSTS], q=10)]\n",
    "best_ix = set(best_damage_df[\"scenario\"].tolist()) & set(best_deaths_df[\"scenario\"].tolist()) # & set(best_cost_df[\"scenario\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_case = results.iloc[list(best_ix)].sample(n=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_damage_df = results.loc[(results[DAMAGE] > np.percentile(a=outcomes[DAMAGE], q=45)) & (results[DAMAGE] <= np.percentile(a=outcomes[DAMAGE], q=55))]\n",
    "middle_deaths_df = results.loc[(results[DEATHS] > np.percentile(a=outcomes[DEATHS], q=45)) & (results[DEATHS] <= np.percentile(a=outcomes[DEATHS], q=55))]\n",
    "middle_cost_df = results.loc[(results[COSTS] > np.percentile(a=outcomes[COSTS], q=45)) & (results[COSTS] <= np.percentile(a=outcomes[COSTS], q=55))]\n",
    "middle_ix =set(middle_damage_df[\"scenario\"].tolist()) & set(middle_deaths_df[\"scenario\"].tolist()) # & set(middle_cost_df[\"scenario\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_case = results.iloc[list(middle_ix)].sample(n=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainties = list(dike_model.uncertainties._data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.concat([middle_case.loc[:, uncertainties], best_case.loc[:, uncertainties], worst_case.loc[:, uncertainties]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = {0: \"middle\", 1: \"best\", 2: \"worst\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [Scenario(f\"{index}\", **row) for index, row in selected.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypervolumemin = [0,0,0]\n",
    "hypervolumemax = outcomes_df[outcomekeys].max(axis=0).values.tolist()\n",
    "max_cost = 1e7\n",
    "hypervolumemax[2] = max_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] generation 0: 0/2500 nfe\n"
     ]
    }
   ],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "nfe = 2500 # + 1000\n",
    "\n",
    "def optimize(scenario, nfe, model, converge_metrics, epsilons):\n",
    "\n",
    "\n",
    "    with MultiprocessingEvaluator(model) as evaluator:\n",
    "        results, convergence = evaluator.optimize(nfe=nfe, searchover='levers',\n",
    "                                     convergence=convergence_metrics,\n",
    "                                     epsilons=epsilons,\n",
    "                                     reference=scenario, convergence_freq=200)\n",
    "    return results, convergence\n",
    "\n",
    "\n",
    "results = []\n",
    "for scenario in scenarios:\n",
    "    convergence_metrics = [HyperVolume(minimum=hypervolumemin, maximum=hypervolumemax),\n",
    "                           EpsilonProgress()]\n",
    "    epsilons = [1e3] * len(dike_model.outcomes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    results.append(optimize(scenario, nfe, dike_model, convergence_metrics, epsilons))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2,sharex=True)\n",
    "for i, (_, convergence) in enumerate(results):\n",
    "    ax1.plot(convergence.nfe, convergence.hypervolume, label=f'scenario {i}')\n",
    "    ax2.plot(convergence.nfe, convergence.epsilon_progress, label=f'scenario {i}')\n",
    "\n",
    "ax1.set_ylabel('hypervolume')\n",
    "ax1.set_xlabel('nfe')\n",
    "ax2.set_ylabel('$\\epsilon$ progress')\n",
    "ax2.set_xlabel('nfe')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df = pd.DataFrame()\n",
    "conv_df = pd.DataFrame()\n",
    "for i, (result, convergence) in enumerate(results):\n",
    "    result.to_csv(\"data/optimisation/\" + ACTORNAME + \"/results_\" + cases[i] +\".csv\", index=False)\n",
    "    convergence.to_csv(\"data/optimisation/\" + ACTORNAME + \"/convergence_\" + cases[i] +\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.to_csv(\"data/optimisation/\" + ACTORNAME + \"/selected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results = []\n",
    "\n",
    "for _,case in cases.items():\n",
    "    temp = pd.read_csv(\"data/optimisation/\" + ACTORNAME + \"/results_\" + case +\".csv\")\n",
    "    temp_ = pd.read_csv(\"data/optimisation/\" + ACTORNAME + \"/convergence_\" + case +\".csv\")\n",
    "    read_results.append([temp, temp_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df = pd.DataFrame()\n",
    "for i, (result, convergence) in enumerate(results):\n",
    "    opt_df = pd.concat([opt_df, result], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyoutcomes = opt_df.loc[:, outcomekeys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot per scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = iter(sns.color_palette())\n",
    "limits = parcoords.get_limits(policyoutcomes)\n",
    "\n",
    "# limits.loc[0, ['inertia', 'reliability']] = 1\n",
    "# limits.loc[0, 'max_P'] = 4 # max over results based on quick inspection not shown here\n",
    "# limits.loc[0, 'utility'] = 1 # max over results based on quick inspection not shown here\n",
    "# limits.loc[1, :] = 0\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "\n",
    "for i, (result, _) in enumerate(results):\n",
    "    color = next(colors)\n",
    "    data = result.loc[:,  outcomekeys]\n",
    "    paraxes.plot(data, label=f'scenario {cases[i]}', color=color)\n",
    "\n",
    "paraxes.legend()\n",
    "plt.rcParams[\"figure.figsize\"] = (5,15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate under deep uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levers = [lever.name for lever in dike_model.levers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = []\n",
    "for i, (result, _) in enumerate(results):\n",
    "    result = result.loc[:, levers]\n",
    "    for j, row in result.iterrows():\n",
    "        policy = Policy(f'scenario {cases[i]} option {j}', **row.to_dict())\n",
    "        policies.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    reevaluation_results = evaluator.perform_experiments(1000, policies=policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du_experiments, du_outcomes = reevaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(reevaluation_results, \"data/optimisation/du_scen_\" + ACTORNAME + \".tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du_experiments, du_outcomes = load_results(\"data/optimisation/du_scen_\" + ACTORNAME + \".tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret\n",
    "Comparing alternatives\n",
    "\"With a regret view, a decision maker wants to minimize the regret of choosing incorrectly, where regret is the loss in performance. This regret could be the cost of assuming the wrong future scenario or the cost of choosing the wrong alternative. In the first case, maximum regret of an alternative is the difference between its performances in the best estimate future scenario and in the scenario where it shows the worst performance, for example, as applied by Kasprzyk et al. (2013). In the second case, the regret of an alternative in a certain future scenario is the difference between its performance and the best-performing alternative, as proposed by Savage (1951). Maximum regret of an alternative is its highest regret achieved over all future scenarios. In both regret cases, the alternative with the smallest maximum regret is the most robust.\"\n",
    "\n",
    "https://link.springer.com/article/10.1007/s13595-017-0641-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kwakkels stuff - still have to test and adept but laptop sloooow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I may have accidentally meddled with this. Github is annoying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_scores = {}\n",
    "regret = []\n",
    "for scenario in du_experiments.scenario.unique():\n",
    "    logical = du_experiments.scenario==scenario\n",
    "    temp_results = {k:v[logical] for k,v in du_outcomes.items()}\n",
    "    temp_results = pd.DataFrame(temp_results)\n",
    "    temp_experiments = du_experiments[du_experiments.scenario==scenario]\n",
    "    \n",
    "    best = temp_results.max()\n",
    "    best['max_P'] = temp_results[outcomekeys[0]].min()\n",
    "    scenario_regret = scenario - best\n",
    "    scenario_regret['policy'] = temp_experiments.policy.values\n",
    "    regret.append(scenario_regret.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret = pd.DataFrame(regret)\n",
    "regret['policy'] = regret['policy'].astype(str)\n",
    "maxregret = regret.groupby('policy').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = parcoords.get_limits(maxregret)\n",
    "paraxes = parcoords.ParallelAxes(maxregret)\n",
    "paraxes.plot(maxregret)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satisficing\n",
    "Performance threshold\n",
    "\"seeks a decision alternative that meets his or her performance requirements over the range of plausible future scenarios. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
