{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4766754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import Scenario, Policy, MultiprocessingEvaluator, ema_logging, load_results\n",
    "from ema_workbench.analysis import prim\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume,\n",
    "                                                     EpsilonProgress)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ema_workbench.analysis import parcoords\n",
    "import seaborn as sns\n",
    "import funs_project as fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583cbb2",
   "metadata": {},
   "source": [
    "# Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d598af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTORNAME = \"Deventer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dike_model, planning_steps = get_model_for_problem_formulation(ACTORNAME)\n",
    "outcomekeys = [outcome.name for outcome in dike_model.outcomes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726f2e0",
   "metadata": {},
   "source": [
    "## Reading\n",
    "The results from the optimization. The policies that are selected with the cure_policy_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = fp.the_cases(ACTORNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = fp.crude_policy_selection(ACTORNAME, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = policies.sort_values(by=\"scenario\")\n",
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policies.iloc[:, 0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abe44d",
   "metadata": {},
   "source": [
    "Below we plot per scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6674700",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = iter(sns.color_palette())\n",
    "limits = parcoords.get_limits(policies.iloc[:, 10:13])\n",
    "\n",
    "# limits.loc[0, ['inertia', 'reliability']] = 1\n",
    "# limits.loc[0, 'max_P'] = 4 # max over results based on quick inspection not shown here\n",
    "# limits.loc[0, 'utility'] = 1 # max over results based on quick inspection not shown here\n",
    "# limits.loc[1, :] = 0\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "\n",
    "for index, row in policies.iterrows():\n",
    "    color = next(colors)\n",
    "    paraxes.plot(row, label=f'scenario {cases[row.scenario]}', color=color)\n",
    "\n",
    "paraxes.legend()\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e981026",
   "metadata": {},
   "source": [
    "# Reevaluate under deep uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bd017",
   "metadata": {},
   "source": [
    "## Reading\n",
    "Read in the results from the reevaluate under deep uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b61f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenarios = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "du_experiments, du_outcomes = load_results(\"simulation/optimisation/du_scen_\" + str(n_scenarios) + \"_\" + ACTORNAME + \".tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb08012",
   "metadata": {},
   "source": [
    "### Keep only the DU experiments and outcomes that are in POLICIES\n",
    "This way only the selected policies (and the respective scenarios) are brought into the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e06683",
   "metadata": {},
   "outputs": [],
   "source": [
    "du_outcomes_df = pd.DataFrame.from_dict(du_outcomes)\n",
    "merged_du = pd.concat([du_experiments, du_outcomes_df], axis=1)\n",
    "merged_du.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ba9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now compare the policies in merged_du to the policies in 'policies' \n",
    "policies_policy_df = policies.iloc[:, 0:10]\n",
    "DU_policy_selected_df = pd.merge(policies_policy_df, merged_du, how = \"inner\")\n",
    "DU_policy_selected_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10059113",
   "metadata": {},
   "source": [
    "#### rewrite du_experiments and du_outcomes with the filtered version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af289528",
   "metadata": {},
   "outputs": [],
   "source": [
    "du_outcomes_policy_df = DU_policy_selected_df[DU_policy_selected_df.columns[-3:]].copy()\n",
    "du_experiments = DU_policy_selected_df[DU_policy_selected_df.columns[0:50]].copy()\n",
    "du_outcomes = du_outcomes_policy_df.to_dict('series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac6906",
   "metadata": {},
   "source": [
    "## Regret\n",
    "Comparing alternatives\n",
    "\"With a regret view, a decision maker wants to minimize the regret of choosing incorrectly, where regret is the loss in performance. This regret could be the cost of assuming the wrong future scenario or the cost of choosing the wrong alternative. In the first case, maximum regret of an alternative is the difference between its performances in the best estimate future scenario and in the scenario where it shows the worst performance, for example, as applied by Kasprzyk et al. (2013). In the second case, the regret of an alternative in a certain future scenario is the difference between its performance and the best-performing alternative, as proposed by Savage (1951). Maximum regret of an alternative is its highest regret achieved over all future scenarios. In both regret cases, the alternative with the smallest maximum regret is the most robust.\"\n",
    "\n",
    "https://link.springer.com/article/10.1007/s13595-017-0641-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomekeys = list(du_outcomes.keys())\n",
    "outcomekeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAMAGE = outcomekeys[0]\n",
    "DEATHS = outcomekeys[1]\n",
    "COSTS = outcomekeys[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede3505",
   "metadata": {},
   "source": [
    "#### Regret calculation based on assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72948936",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_scores = {}\n",
    "regret = []\n",
    "for scenario in du_experiments.scenario.unique():\n",
    "    logical = du_experiments.scenario==scenario\n",
    "    temp_results = {k:v[logical] for k,v in du_outcomes.items()}\n",
    "    temp_results = pd.DataFrame(temp_results)\n",
    "    temp_experiments = du_experiments[du_experiments.scenario==scenario]\n",
    "    best = temp_results.min() #we are minimizing\n",
    "    scenario_regret = temp_results - best\n",
    "    scenario_regret['policy'] = temp_experiments.policy.values\n",
    "    regret.append(scenario_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regret = pd.concat(regret)\n",
    "maxregret = regret.groupby('policy').max().dropna()#dropna to remove some artifacts that were left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dee018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxregret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ef4b7",
   "metadata": {},
   "source": [
    "### renaming policies for legend and colourssss\n",
    "rename policy names to things that will stick and we'll remember\n",
    "In the order that they are above, replace with \n",
    "D_0, D_1, ...  D_10, D_11,\n",
    "\n",
    "At the same time create a dictionary for the colours that is linked to the policy name, so that for the following graphs, the same policies have the same colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d15cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming and colour block\n",
    "old_names = maxregret.index.values.tolist()\n",
    "new_names = []\n",
    "dict_naming = {}\n",
    "\n",
    "for i in range(len(maxregret)):\n",
    "    name = \"D_\" + str(i)\n",
    "    #print(name)\n",
    "    new_names.append(name)\n",
    "dict_naming = {old_names[i]: new_names[i] for i in range(len(old_names))}\n",
    "#dict_naming  \n",
    "\n",
    "#Dictionary for the coloursss\n",
    "color_list =  sns.color_palette('Spectral',len(old_names))\n",
    "dict_colours = {old_names[i]: color_list[i] for i in range(len(old_names))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "limits = parcoords.get_limits(maxregret)\n",
    "paraxes = parcoords.ParallelAxes(maxregret)\n",
    "\n",
    "\n",
    "for index, row in maxregret.iterrows():\n",
    "    paraxes.plot(row, color=dict_colours[index], label=dict_naming[index])\n",
    "    \n",
    "paraxes.legend()\n",
    "plt.figtext(.5,1,'Regret results - Deventer',fontsize=25,ha='center')\n",
    "\n",
    "plt.savefig(\"../../report/figures/results/regret_figure\"+ \"_\" + ACTORNAME+ \".png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a2e9c",
   "metadata": {},
   "source": [
    "## Satisficing\n",
    "Performance threshold\n",
    "\"seeks a decision alternative that meets his or her performance requirements over the range of plausible future scenarios. \"\n",
    "\n",
    "For the satisficing analysis, the domain criterion metric from the assignments and as discussed in the paper: [(Bartholomew, Kwakkel 2020)](https://repository.tudelft.nl/islandora/object/uuid%3A17668d72-4ae4-47a4-9905-ebb0e1e75128). Another option that we explored was looking over the policies and seeing if there are any policies that are within all three thresholds. \n",
    "\n",
    "The threshold values are found within funs_project.py and sources can be found within the acompanying report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "satisfycing_df = policies.copy()\n",
    "satisfycing_df.drop_duplicates(inplace=True)\n",
    "#satisfycing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58096059",
   "metadata": {},
   "outputs": [],
   "source": [
    "belowtresh_df = satisfycing_df.loc[(satisfycing_df['Deventer Expected Annual Damage'] < fp.thresholds_deventer['Deventer Expected Annual Damage']) & \\\n",
    "                    (satisfycing_df['Deventer Expected Number of Deaths'] < fp.thresholds_deventer['Deventer Expected Number of Deaths']) & \\\n",
    "                    (satisfycing_df['Deventer Total Costs'] < fp.thresholds_deventer['Deventer Total Costs']),:].copy()\n",
    "belowtresh_df.sort_index(inplace=True, ascending = False)\n",
    "belowtresh_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c91b5",
   "metadata": {},
   "source": [
    "From the 12 selected policies there are two policies that fall completly within all the thresholds for Deventer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2e6dd",
   "metadata": {},
   "source": [
    "### Domain-criterion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5993290",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits_df = belowtresh_df.iloc[:, -4:-1]\n",
    "colors = iter(sns.color_palette())\n",
    "limits = parcoords.get_limits(limits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b97559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overall_scores = {}\n",
    "for policy in du_experiments.policy.unique():\n",
    "    logical = du_experiments.policy == policy\n",
    "    scores = {}\n",
    "    for k, v in du_outcomes.items():\n",
    "        try:\n",
    "            n = np.sum(v[logical]<=fp.thresholds_deventer[k])#check if it is below thresholds\n",
    "        except KeyError:\n",
    "            continue\n",
    "        scores[k] = n/1000 \n",
    "    overall_scores[policy] = scores\n",
    "        \n",
    "overall_scores = pd.DataFrame(overall_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "limits = parcoords.get_limits(overall_scores)\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "for index, row in overall_scores.iterrows():\n",
    "    paraxes.plot(row, color=dict_colours[index], label=dict_naming[index])\n",
    "    \n",
    "paraxes.legend()\n",
    "plt.figtext(.5,1,'Satisficing results - Deventer',fontsize=25,ha='center')\n",
    "\n",
    "plt.savefig(\"../../report/figures/results/domain_criterion\"+ \"_\" + ACTORNAME+ \".png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5360c",
   "metadata": {},
   "source": [
    "## Scoring policies\n",
    "This part combines the results from both robustness measures to score the policies and select the most robust 5 policies. The regret results are normalised first to make it possible to get a 'good' average. This will be done by first taking the average of the two metrics and than sorting first by the domain-criterion and then by regret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9b770",
   "metadata": {},
   "source": [
    "### Normalise the regret and take the average to score the policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the regret results\n",
    "regret_scores = maxregret.copy()\n",
    "regret_average = regret_scores.apply(lambda x: x/x.max(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average for each policy\n",
    "regret_average['average regret'] = regret_average.mean(numeric_only=True, axis=1)\n",
    "regret_average.sort_values(by='average regret', ascending = False, inplace = True) #Lower = better\n",
    "regret_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee31c61",
   "metadata": {},
   "source": [
    "###  Use the satisficing / domain criterion and take their average to score the policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de409e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average for each policy\n",
    "satisficing_average = overall_scores.copy()\n",
    "satisficing_average['average satisficing'] = satisficing_average.mean(numeric_only=True, axis=1)\n",
    "satisficing_average.sort_values(by='average satisficing', ascending = True, inplace = True) #higher = better\n",
    "satisficing_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e2f7d",
   "metadata": {},
   "source": [
    "### Add them together  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111687cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([satisficing_average, regret_average], join=\"inner\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca95ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values(by='average satisficing', inplace = True, ascending = True)\n",
    "merged_df.sort_values(by='average regret', inplace = True, ascending = False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff343603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take te most robust policies and put them into a df.\n",
    "robust_policies = merged_df.tail(5)\n",
    "robust_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061e7f4",
   "metadata": {},
   "source": [
    "The policies above show that there often is a trade-off between satisficing and regret. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a47af",
   "metadata": {},
   "source": [
    "## Tying it back to the policy levers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to return to an original list of policies with this\n",
    "policy_names = robust_policies.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_results = DU_policy_selected_df[DU_policy_selected_df['policy'].isin(policy_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12084b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lever_names = policies_policy_df.columns.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_policies_results = temp_results[lever_names].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08db71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_policies_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86912b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_policies_results.to_csv('simulation/selected/selected_policies_' + ACTORNAME + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d78773",
   "metadata": {},
   "outputs": [],
   "source": [
    "lever_names.append('policy')\n",
    "robust_policies_results_names = temp_results[lever_names].drop_duplicates()\n",
    "robust_policies_results_names.set_index('policy', inplace = True)\n",
    "robust_policies_results_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59073de",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_policies_results_names[\"new_policy_name\"] = np.nan\n",
    "for index, row in robust_policies_results_names.iterrows():\n",
    "    robust_policies_results_names.at[index, \"new_policy_name\"] = dict_naming[index]\n",
    "robust_policies_results_names.to_csv('simulation/selected/selected_policies_NAMES_' + ACTORNAME + '.csv')\n",
    "robust_policies_results_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
