{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intended-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import Scenario, Policy, MultiprocessingEvaluator, ema_logging, load_results\n",
    "from ema_workbench.analysis import prim\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume,\n",
    "                                                     EpsilonProgress)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ema_workbench.analysis import parcoords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twenty-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "dike_model, planning_steps = get_model_for_problem_formulation(\"Gorssel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-attempt",
   "metadata": {},
   "source": [
    "# Scenario selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "editorial-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR = \"genscen_Gorssel_50000_09-06-2021-01-38-16.tar.gz\"\n",
    "experiments, outcomes = load_results(\"data/generated/\" + ACTOR)\n",
    "outcomes_df = pd.DataFrame(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infinite-honor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Difference in Expected Annual Damage Gorssel-Deventer',\n",
       " 'Difference in Expected Number of Deaths Gorssel-Deventer',\n",
       " 'Gorssel Expected Annual Damage',\n",
       " 'Gorssel Expected Number of Deaths',\n",
       " 'Total Costs of Policies']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "duplicate-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAMAGE = outcomes_df.columns[2]\n",
    "DEATHS = outcomes_df.columns[3]\n",
    "COSTS = outcomes_df.columns[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atlantic-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([experiments, outcomes_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polished-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_damage_df = results.loc[results[DAMAGE] > np.percentile(a=outcomes[DAMAGE], q=90)]\n",
    "worst_deaths_df = results.loc[results[DEATHS] > np.percentile(a=outcomes[DEATHS], q=90)]\n",
    "worst_cost_df = results.loc[results[COSTS] > np.percentile(a=outcomes[COSTS], q=90)]\n",
    "worst_ix =set(worst_damage_df[\"scenario\"].tolist()) & set(worst_deaths_df[\"scenario\"].tolist()) & set(worst_cost_df[\"scenario\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changing-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_case = results.iloc[list(worst_ix)].sample(n=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "patent-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_damage_df = results.loc[results[DAMAGE] <= np.percentile(a=outcomes[DAMAGE], q=10)]\n",
    "best_deaths_df = results.loc[results[DEATHS] <= np.percentile(a=outcomes[DEATHS], q=10)]\n",
    "best_cost_df = results.loc[results[COSTS] <= np.percentile(a=outcomes[COSTS], q=10)]\n",
    "best_ix = set(best_damage_df[\"scenario\"].tolist()) & set(best_deaths_df[\"scenario\"].tolist()) & set(best_cost_df[\"scenario\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instant-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_case = results.iloc[list(best_ix)].sample(n=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sacred-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_damage_df = results.loc[(results[DAMAGE] > np.percentile(a=outcomes[DAMAGE], q=45)) & (results[DAMAGE] <= np.percentile(a=outcomes[DAMAGE], q=55))]\n",
    "middle_deaths_df = results.loc[(results[DEATHS] > np.percentile(a=outcomes[DEATHS], q=45)) & (results[DEATHS] <= np.percentile(a=outcomes[DEATHS], q=55))]\n",
    "middle_cost_df = results.loc[(results[COSTS] > np.percentile(a=outcomes[COSTS], q=45)) & (results[COSTS] <= np.percentile(a=outcomes[COSTS], q=55))]\n",
    "middle_ix =set(middle_damage_df[\"scenario\"].tolist()) & set(middle_deaths_df[\"scenario\"].tolist()) & set(middle_cost_df[\"scenario\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "graduate-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_case = results.iloc[list(middle_ix)].sample(n=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aboriginal-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainties =list(dike_model.uncertainties._data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "annual-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discount rate 0</th>\n",
       "      <th>discount rate 1</th>\n",
       "      <th>discount rate 2</th>\n",
       "      <th>A.0_ID flood wave shape</th>\n",
       "      <th>A.1_Bmax</th>\n",
       "      <th>A.1_pfail</th>\n",
       "      <th>A.1_Brate</th>\n",
       "      <th>A.2_Bmax</th>\n",
       "      <th>A.2_pfail</th>\n",
       "      <th>A.2_Brate</th>\n",
       "      <th>...</th>\n",
       "      <th>2_RfR 2</th>\n",
       "      <th>A.1_DikeIncrease 0</th>\n",
       "      <th>A.1_DikeIncrease 1</th>\n",
       "      <th>A.1_DikeIncrease 2</th>\n",
       "      <th>A.2_DikeIncrease 0</th>\n",
       "      <th>A.2_DikeIncrease 1</th>\n",
       "      <th>A.2_DikeIncrease 2</th>\n",
       "      <th>A.3_DikeIncrease 0</th>\n",
       "      <th>A.3_DikeIncrease 1</th>\n",
       "      <th>A.3_DikeIncrease 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>108.796933</td>\n",
       "      <td>0.175097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>274.649688</td>\n",
       "      <td>0.546471</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     discount rate 0 discount rate 1 discount rate 2  A.0_ID flood wave shape  \\\n",
       "5569             3.5             3.5             2.5                     82.0   \n",
       "\n",
       "        A.1_Bmax  A.1_pfail A.1_Brate    A.2_Bmax  A.2_pfail A.2_Brate  ...  \\\n",
       "5569  108.796933   0.175097       1.0  274.649688   0.546471      10.0  ...   \n",
       "\n",
       "      2_RfR 2  A.1_DikeIncrease 0 A.1_DikeIncrease 1  A.1_DikeIncrease 2  \\\n",
       "5569      1.0                 5.0                5.0                 3.0   \n",
       "\n",
       "      A.2_DikeIncrease 0 A.2_DikeIncrease 1  A.2_DikeIncrease 2  \\\n",
       "5569                 8.0                1.0                10.0   \n",
       "\n",
       "      A.3_DikeIncrease 0 A.3_DikeIncrease 1  A.3_DikeIncrease 2  \n",
       "5569                 7.0                5.0                 4.0  \n",
       "\n",
       "[1 rows x 37 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_case.loc[:, uncertainties]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "funny-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.concat([middle_case.loc[:, uncertainties], best_case.loc[:, uncertainties], worst_case.loc[:, uncertainties]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collected-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [Scenario(f\"{index}\", **row) for index, row in selected.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "excited-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypervolumemin = outcomes_df[[DAMAGE, DEATHS, COSTS]].min().values\n",
    "hypervolumemax = outcomes_df[[DAMAGE, DEATHS, COSTS]].max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cooperative-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149113374.85624802"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypervolumemin[2]-1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "apart-excitement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 1.50113375e+08])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypervolumemin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "varied-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 2.56875154e+08, 8.17731834e-02,\n",
       "       1.74721032e+09])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_df.max(axis=0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-instrument",
   "metadata": {},
   "source": [
    "# Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-satellite",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] generation 0: 0/150 nfe\n"
     ]
    }
   ],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "nfe = 3000\n",
    "\n",
    "def optimize(scenario, nfe, model, converge_metrics, epsilons):\n",
    "\n",
    "\n",
    "    with MultiprocessingEvaluator(model) as evaluator:\n",
    "        results, convergence = evaluator.optimize(nfe=nfe, searchover='levers',\n",
    "                                     convergence=convergence_metrics,\n",
    "                                     epsilons=epsilons,\n",
    "                                     reference=scenario, convergence_freq=200)\n",
    "    return results, convergence\n",
    "\n",
    "\n",
    "results = []\n",
    "for scenario in scenarios:\n",
    "    # need to pick better hypervolume limits\n",
    "    convergence_metrics = [HyperVolume(minimum=hypervolumemin, maximum=hypervolumemax),\n",
    "                           EpsilonProgress()]\n",
    "    epsilons = [1e3] * len(dike_model.outcomes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    results.append(optimize(scenario, nfe, dike_model, convergence_metrics, epsilons))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-syria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2,sharex=True)\n",
    "for i, (_, convergence) in enumerate(results):\n",
    "    ax1.plot(convergence.nfe, convergence.hypervolume, label=f'scenario {i}')\n",
    "    ax2.plot(convergence.nfe, convergence.epsilon_progress, label=f'scenario {i}')\n",
    "\n",
    "plt.xlim(0,500)\n",
    "# plt.xlim(0,500)\n",
    "ax1.set_ylabel('hypervolume')\n",
    "ax1.set_xlabel('nfe')\n",
    "ax2.set_ylabel('$\\epsilon$ progress')\n",
    "ax2.set_xlabel('nfe')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_df= pd.DataFrame()\n",
    "\n",
    "cases= {0: \"middle case\", 1:\"best case\", 2: \"worst case\"}\n",
    "\n",
    "for i, (result, _) in enumerate(results):\n",
    "    result[\"scenario\"] = cases[i]\n",
    "    outcomes_df = pd.concat([outcomes_df, result], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [outcome.name for outcome in dike_model.outcomes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = outcomes[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "policyoutcomes = outcomes_df.loc[:, outcomes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "policyoutcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-consistency",
   "metadata": {},
   "source": [
    "Below we plot per scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = iter(sns.color_palette())\n",
    "limits = parcoords.get_limits(policyoutcomes)\n",
    "\n",
    "# limits.loc[0, ['inertia', 'reliability']] = 1\n",
    "# limits.loc[0, 'max_P'] = 4 # max over results based on quick inspection not shown here\n",
    "# limits.loc[0, 'utility'] = 1 # max over results based on quick inspection not shown here\n",
    "# limits.loc[1, :] = 0\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "\n",
    "for i, (result, _) in enumerate(results):\n",
    "    color = next(colors)\n",
    "    data = result.loc[:,  outcomes]\n",
    "    paraxes.plot(data, label=f'scenario {cases[i]}', color=color)\n",
    "\n",
    "paraxes.legend()\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-surface",
   "metadata": {},
   "source": [
    "# Re-evaluate under deep uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "levers = [lever.name for lever in dike_model.levers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = []\n",
    "for i, (result, _) in enumerate(results):\n",
    "    result = result.loc[:, levers]\n",
    "    for j, row in result.iterrows():\n",
    "        policy = Policy(f'scenario {cases[i]} option {j}', **row.to_dict())\n",
    "        policies.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    reeevaluation_results = evaluator.perform_experiments(1000, policies=policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-bride",
   "metadata": {},
   "source": [
    "## Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = pd.DataFrame(results[0][0])\n",
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = pd.DataFrame(results[1][0])\n",
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-eating",
   "metadata": {},
   "source": [
    "### Kwakkels stuff - still have to test and adept but laptop sloooow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = reeevaluation_results\n",
    "overall_scores = {}\n",
    "regret = []\n",
    "for scenario in experiments.scenario.unique():\n",
    "    logical = experiments.scenario==scenario\n",
    "    temp_results = {k:v[logical] for k,v in outcomes.items()}\n",
    "    temp_results = pd.DataFrame(temp_results)\n",
    "    temp_experiments = experiments[experiments.scenario==scenario]\n",
    "        \n",
    "    best = temp_results.max()\n",
    "    best['max_P'] = temp_results.max_P.min()\n",
    "    scenario_regret = a - best\n",
    "    scenario_regret['policy'] = temp_experiments.policy.values    \n",
    "    regret.append(scenario_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "regret = pd.concat(regret)\n",
    "maxregret = regret.groupby('policy').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = parcoords.get_limits(maxregret)\n",
    "paraxes = parcoords.ParallelAxes(maxregret)\n",
    "paraxes.plot(maxregret)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-superintendent",
   "metadata": {},
   "source": [
    "## Satisficing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
